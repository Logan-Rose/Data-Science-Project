{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conduct supervised learning using a label of your own choice. That is, you are required to identify your own classification task. For instance, you may  consider using one of the following attributes as your class label: Quality-of-Life, Development-Index, Human-Development-Index, Gender, Age-group, and so on. For instance, suppose you choose to focus on the Development-Index. In this case, you would construct data mining models that contrast the trends in the countries using class labels in {developed, developing and underdeveloped}. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import psycopg2\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime\n",
    "from sklearn.model_selection import train_test_split, StratifiedShuffleSplit, cross_val_score, GridSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "RANDOM_STATE = 42\n",
    "\n",
    "conn = psycopg2.connect(\n",
    "    host=\"localhost\",\n",
    "    database=\"postgres\",\n",
    "    user=\"postgres\",\n",
    "    password=\"password\")\n",
    "cur = conn.cursor()\n",
    "tables = [\n",
    "  'country',\n",
    "  'education',\n",
    "  'event',\n",
    "  'fact',\n",
    "  'health',\n",
    "  'population',\n",
    "  'quality_of_life',\n",
    "  'month'\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getTable(name):\n",
    "  cur.execute('SELECT * from ' + name)\n",
    "  return pd.DataFrame(cur.fetchall(), columns=[desc[0] for desc in cur.description])\n",
    "\n",
    "def getAll(tables):\n",
    "  query = 'SELECT'\n",
    "  for (i,table) in enumerate(tables):\n",
    "    if table !='fact':\n",
    "      query+= ' ' + table + '.*'\n",
    "      if i != len(tables)-1:\n",
    "        query += ','\n",
    "    else:\n",
    "      query += 'fact.hdi'\n",
    "  query += ' FROM Fact,'\n",
    "  for (i,table) in enumerate(tables):\n",
    "    if table !='fact':\n",
    "      query+= table\n",
    "      if i != len(tables)-1:\n",
    "        query += ', '\n",
    "      else:\n",
    "        query += ' '\n",
    "  query += 'WHERE '\n",
    "  for (i,table) in enumerate(tables):\n",
    "    if table !='fact':\n",
    "      if table == 'month':\n",
    "        query += 'Fact.date_key = month.key'\n",
    "      else:\n",
    "        query += 'Fact.'+table+'_key = ' + table + '.key'\n",
    "      if i != len(tables)-1:\n",
    "        query += ' and '\n",
    "  print(query)\n",
    "  cur.execute(query)\n",
    "  return pd.DataFrame(cur.fetchall(), columns=[desc[0] for desc in cur.description])\n",
    "\n",
    "\n",
    "def getCorrelation(measure, attribute, factor):\n",
    "  # DISTINCT ? \n",
    "  cur.execute('Select DISTINCT F.' + measure + ', A.' + factor + ' from Fact as F, ' + attribute + ' as A where F.' + attribute + '_key = A.'+ attribute + '_key')\n",
    "  return pd.DataFrame(cur.fetchall(), columns=[desc[0] for desc in cur.description])\n",
    "\n",
    "def getMeasureByAttribute(measure, attribute):\n",
    "  cur.execute('SELECT DISTINCT Fact.' + measure + ', A.* FROM Fact, ' + attribute + ' as A WHERE Fact.' + attribute + '_key = A.'+ attribute + '_key')\n",
    "  return pd.DataFrame(cur.fetchall(), columns=[desc[0] for desc in cur.description])\n",
    "\n",
    "def getDataCorrelation(measure, attribute):\n",
    "  dataset = getMeasureByAttribute(measure, attribute)\n",
    "  dataset = dataset.apply(pd.to_numeric)\n",
    "  dataset[measure] = dataset[measure].round(decimals=1)*10\n",
    "\n",
    "  train, test = train_test_split(dataset, test_size=0.2, random_state=RANDOM_STATE)\n",
    "\n",
    "  train_labels = train[measure].values.reshape(-1, 1)\n",
    "  train_data = train.drop([measure,attribute+'_key'], axis=1).values.reshape(-1, len(train.columns)-2)\n",
    "\n",
    "  test_labels = test[measure].values.reshape(-1, 1)\n",
    "  test_data = test.drop([measure,attribute+'_key'], axis=1).values.reshape(-1, len(test.columns)-2)\n",
    "  return train_labels, train_data, test_labels, test_data\n",
    "\n",
    "tables = [\n",
    "  'country',\n",
    "  'education',\n",
    "  'event',\n",
    "  'fact',\n",
    "  'health',\n",
    "  'population',\n",
    "  'quality_of_life',\n",
    "  'month'\n",
    "]\n",
    "\n",
    "frames = {}\n",
    "for table in tables:\n",
    "  frames[table] = getTable(table)\n",
    "# print(frames)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_labels, train_data, test_labels, test_data = getDataCorrelation('hdi', 'education')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. (15 marks) Use the Decision Tree, Gradient Boosting and Random Forest algorithms to construct models against your data, following the so-called train-then-test, or holdout method.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/logan/miniconda3/envs/csi4106/lib/python3.8/site-packages/sklearn/ensemble/_gb.py:494: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/tmp/ipykernel_9762/1691069752.py:20: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  randomForrest.fit(train_data, train_labels)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DecisionTreeClassifier(max_depth=10, min_samples_split=10, random_state=42)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gradientBoost = GradientBoostingClassifier(\n",
    "  n_estimators=100,\n",
    "  learning_rate=0.01,\n",
    "  max_depth=1,\n",
    "  random_state=RANDOM_STATE\n",
    "  )\n",
    "randomForrest = RandomForestClassifier(\n",
    "  random_state=RANDOM_STATE,\n",
    "  max_depth=10,\n",
    "  min_samples_split=10\n",
    ")\n",
    "\n",
    "decisionTree = DecisionTreeClassifier(\n",
    "  max_depth=10,\n",
    "  min_samples_split=10,\n",
    "  random_state=RANDOM_STATE\n",
    ")\n",
    "\n",
    "gradientBoost.fit(train_data, train_labels)\n",
    "randomForrest.fit(train_data, train_labels)\n",
    "decisionTree.fit(train_data, train_labels)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. (20 marks) Compare the results of the three learning algorithms, in terms of (i) accuracy, (ii) precision, (iii) recall and (iv) time to construct the models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient boost train time: 0:00:00.010064\n",
      "Gradient Boost Report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         4.0       1.00      1.00      1.00         2\n",
      "         5.0       1.00      1.00      1.00         1\n",
      "         6.0       1.00      0.89      0.94         9\n",
      "         7.0       0.50      0.80      0.62         5\n",
      "         8.0       0.00      0.00      0.00         3\n",
      "         9.0       1.00      1.00      1.00         6\n",
      "\n",
      "    accuracy                           0.81        26\n",
      "   macro avg       0.75      0.78      0.76        26\n",
      "weighted avg       0.79      0.81      0.79        26\n",
      "\n",
      "Random Forrest Report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         4.0       1.00      1.00      1.00         2\n",
      "         5.0       1.00      1.00      1.00         1\n",
      "         6.0       1.00      0.89      0.94         9\n",
      "         7.0       0.56      1.00      0.71         5\n",
      "         8.0       0.00      0.00      0.00         3\n",
      "         9.0       1.00      1.00      1.00         6\n",
      "\n",
      "    accuracy                           0.85        26\n",
      "   macro avg       0.76      0.81      0.78        26\n",
      "weighted avg       0.80      0.85      0.81        26\n",
      "\n",
      "Decision Tree Report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         4.0       1.00      1.00      1.00         2\n",
      "         5.0       1.00      1.00      1.00         1\n",
      "         6.0       0.89      0.89      0.89         9\n",
      "         7.0       0.50      0.80      0.62         5\n",
      "         8.0       0.00      0.00      0.00         3\n",
      "         9.0       1.00      1.00      1.00         6\n",
      "\n",
      "    accuracy                           0.81        26\n",
      "   macro avg       0.73      0.78      0.75        26\n",
      "weighted avg       0.75      0.81      0.77        26\n",
      "\n"
     ]
    }
   ],
   "source": [
    "startTime = datetime.now()\n",
    "gradientPredictions = gradientBoost.predict(test_data)\n",
    "gradientPredictionsReport = classification_report(test_labels.flatten(), gradientPredictions, zero_division=0)\n",
    "print('Gradient boost train time:', datetime.now()-startTime )\n",
    "\n",
    "startTime = datetime.now()\n",
    "randomForrestPredictions = randomForrest.predict(test_data)\n",
    "randomForrestPredictionsReport = classification_report(test_labels.flatten(), randomForrestPredictions, zero_division=0)\n",
    "print('Random Forrest train time:', datetime.now()-startTime )\n",
    "\n",
    "startTime = datetime.now()\n",
    "decisionTreePredictions = decisionTree.predict(test_data)\n",
    "decisionTreePredictionsReport = classification_report(test_labels.flatten(), decisionTreePredictions, zero_division=0)\n",
    "print('Decision Tree train time:', datetime.now()-startTime )\n",
    "\n",
    "print('Gradient Boost Report')\n",
    "print(gradientPredictionsReport)\n",
    "print('Random Forrest Report')\n",
    "print(randomForrestPredictionsReport)\n",
    "print('Decision Tree Report')\n",
    "print(decisionTreePredictionsReport)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Metric                   | Gradient | Random Forrest | Decision Tree |\n",
    "|--------------------------|----------|----------------|---------------|\n",
    "| accuracy                 |    81%   |      85%       |     81%       |\n",
    "| precision (weighted avg) |    79%   |      76%       |     73%       |\n",
    "| recall (weighted avg)    |    81%   |      81%       |     78%       |\n",
    "| construction time        |          |                |               |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. (15 marks) Submit a 200 to 300 words summary explaining the actionable knowledge nuggets your team discovered. That is, you should explain what insights you obtained about the data, when investigating the models produced by the three algorithms. "
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
