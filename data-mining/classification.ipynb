{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conduct supervised learning using a label of your own choice. That is, you are required to identify your own classification task. For instance, you may  consider using one of the following attributes as your class label: Quality-of-Life, Development-Index, Human-Development-Index, Gender, Age-group, and so on. For instance, suppose you choose to focus on the Development-Index. In this case, you would construct data mining models that contrast the trends in the countries using class labels in {developed, developing and underdeveloped}. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import psycopg2\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime\n",
    "from sklearn.model_selection import train_test_split, StratifiedShuffleSplit, cross_val_score, GridSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "RANDOM_STATE = 42\n",
    "\n",
    "conn = psycopg2.connect(\n",
    "    host=\"localhost\",\n",
    "    database=\"postgres\",\n",
    "    user=\"postgres\",\n",
    "    password=\"password\")\n",
    "cur = conn.cursor()\n",
    "tables = [\n",
    "  'country',\n",
    "  'education',\n",
    "  'event',\n",
    "  'fact',\n",
    "  'health',\n",
    "  'population',\n",
    "  'quality_of_life',\n",
    "  'month'\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getTable(name):\n",
    "  cur.execute('SELECT * from ' + name)\n",
    "  return pd.DataFrame(cur.fetchall(), columns=[desc[0] for desc in cur.description])\n",
    "\n",
    "def getAll(tables):\n",
    "  query = 'SELECT'\n",
    "  for (i,table) in enumerate(tables):\n",
    "    if table !='fact':\n",
    "      query+= ' ' + table + '.*'\n",
    "      if i != len(tables)-1:\n",
    "        query += ','\n",
    "    else:\n",
    "      query += 'fact.hdi'\n",
    "  query += ' FROM Fact,'\n",
    "  for (i,table) in enumerate(tables):\n",
    "    if table !='fact':\n",
    "      query+= table\n",
    "      if i != len(tables)-1:\n",
    "        query += ', '\n",
    "      else:\n",
    "        query += ' '\n",
    "  query += 'WHERE '\n",
    "  for (i,table) in enumerate(tables):\n",
    "    if table !='fact':\n",
    "      if table == 'month':\n",
    "        query += 'Fact.date_key = month.key'\n",
    "      else:\n",
    "        query += 'Fact.'+table+'_key = ' + table + '.key'\n",
    "      if i != len(tables)-1:\n",
    "        query += ' and '\n",
    "  print(query)\n",
    "  cur.execute(query)\n",
    "  return pd.DataFrame(cur.fetchall(), columns=[desc[0] for desc in cur.description])\n",
    "\n",
    "\n",
    "def getCorrelation(measure, attribute, factor):\n",
    "  # DISTINCT ? \n",
    "  cur.execute('Select DISTINCT F.' + measure + ', A.' + factor + ' from Fact as F, ' + attribute + ' as A where F.' + attribute + '_key = A.'+ attribute + '_key')\n",
    "  return pd.DataFrame(cur.fetchall(), columns=[desc[0] for desc in cur.description])\n",
    "\n",
    "def getMeasureByAttribute(measure, attribute):\n",
    "  cur.execute('SELECT DISTINCT Fact.' + measure + ', A.* FROM Fact, ' + attribute + ' as A WHERE Fact.' + attribute + '_key = A.'+ attribute + '_key')\n",
    "  return pd.DataFrame(cur.fetchall(), columns=[desc[0] for desc in cur.description])\n",
    "\n",
    "def getDataCorrelation(measure, attribute):\n",
    "  dataset = getMeasureByAttribute(measure, attribute)\n",
    "  dataset = dataset.apply(pd.to_numeric)\n",
    "  dataset[measure] = dataset[measure].round(decimals=1)*10\n",
    "\n",
    "  train, test = train_test_split(dataset, test_size=0.2, random_state=RANDOM_STATE)\n",
    "\n",
    "  train_labels = train[measure].values.reshape(-1, 1)\n",
    "  train_data = train.drop([measure,attribute+'_key'], axis=1).values.reshape(-1, len(train.columns)-2)\n",
    "\n",
    "  test_labels = test[measure].values.reshape(-1, 1)\n",
    "  test_data = test.drop([measure,attribute+'_key'], axis=1).values.reshape(-1, len(test.columns)-2)\n",
    "  return train_labels, train_data, test_labels, test_data\n",
    "\n",
    "tables = [\n",
    "  'country',\n",
    "  'education',\n",
    "  'event',\n",
    "  'fact',\n",
    "  'health',\n",
    "  'population',\n",
    "  'quality_of_life',\n",
    "  'month'\n",
    "]\n",
    "\n",
    "frames = {}\n",
    "for table in tables:\n",
    "  frames[table] = getTable(table)\n",
    "# print(frames)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_labels, train_data, test_labels, test_data = getDataCorrelation('hdi', 'population')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. (15 marks) Use the Decision Tree, Gradient Boosting and Random Forest algorithms to construct models against your data, following the so-called train-then-test, or holdout method.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/logan/miniconda3/envs/csi4106/lib/python3.8/site-packages/sklearn/ensemble/_gb.py:494: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/tmp/ipykernel_9762/1691069752.py:20: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  randomForrest.fit(train_data, train_labels)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DecisionTreeClassifier(max_depth=10, min_samples_split=10, random_state=42)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gradientBoost = GradientBoostingClassifier(\n",
    "  n_estimators=100,\n",
    "  learning_rate=0.01,\n",
    "  max_depth=1,\n",
    "  random_state=RANDOM_STATE\n",
    "  )\n",
    "randomForrest = RandomForestClassifier(\n",
    "  random_state=RANDOM_STATE,\n",
    "  max_depth=10,\n",
    "  min_samples_split=10\n",
    ")\n",
    "\n",
    "decisionTree = DecisionTreeClassifier(\n",
    "  max_depth=10,\n",
    "  min_samples_split=10,\n",
    "  random_state=RANDOM_STATE\n",
    ")\n",
    "\n",
    "gradientBoost.fit(train_data, train_labels)\n",
    "randomForrest.fit(train_data, train_labels)\n",
    "decisionTree.fit(train_data, train_labels)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. (20 marks) Compare the results of the three learning algorithms, in terms of (i) accuracy, (ii) precision, (iii) recall and (iv) time to construct the models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "X has 16 features, but GradientBoostingClassifier is expecting 12 features as input.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[0;32mIn [40]\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m startTime \u001b[38;5;241m=\u001b[39m datetime\u001b[38;5;241m.\u001b[39mnow()\n\u001b[0;32m----> 2\u001b[0m gradientPredictions \u001b[38;5;241m=\u001b[39m \u001b[43mgradientBoost\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtest_data\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m gradientPredictionsReport \u001b[38;5;241m=\u001b[39m classification_report(test_labels\u001b[38;5;241m.\u001b[39mflatten(), gradientPredictions, zero_division\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mGradient boost train time:\u001b[39m\u001b[38;5;124m'\u001b[39m, datetime\u001b[38;5;241m.\u001b[39mnow()\u001b[38;5;241m-\u001b[39mstartTime )\n",
      "File \u001b[0;32m~/miniconda3/envs/csi4106/lib/python3.8/site-packages/sklearn/ensemble/_gb.py:1359\u001b[0m, in \u001b[0;36mGradientBoostingClassifier.predict\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m   1344\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpredict\u001b[39m(\u001b[38;5;28mself\u001b[39m, X):\n\u001b[1;32m   1345\u001b[0m     \u001b[38;5;124;03m\"\"\"Predict class for X.\u001b[39;00m\n\u001b[1;32m   1346\u001b[0m \n\u001b[1;32m   1347\u001b[0m \u001b[38;5;124;03m    Parameters\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1357\u001b[0m \u001b[38;5;124;03m        The predicted values.\u001b[39;00m\n\u001b[1;32m   1358\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1359\u001b[0m     raw_predictions \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecision_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1360\u001b[0m     encoded_labels \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloss_\u001b[38;5;241m.\u001b[39m_raw_prediction_to_decision(raw_predictions)\n\u001b[1;32m   1361\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclasses_\u001b[38;5;241m.\u001b[39mtake(encoded_labels, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/csi4106/lib/python3.8/site-packages/sklearn/ensemble/_gb.py:1312\u001b[0m, in \u001b[0;36mGradientBoostingClassifier.decision_function\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m   1293\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecision_function\u001b[39m(\u001b[38;5;28mself\u001b[39m, X):\n\u001b[1;32m   1294\u001b[0m     \u001b[38;5;124;03m\"\"\"Compute the decision function of ``X``.\u001b[39;00m\n\u001b[1;32m   1295\u001b[0m \n\u001b[1;32m   1296\u001b[0m \u001b[38;5;124;03m    Parameters\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1310\u001b[0m \u001b[38;5;124;03m        array of shape (n_samples,).\u001b[39;00m\n\u001b[1;32m   1311\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1312\u001b[0m     X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_validate_data\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1313\u001b[0m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mDTYPE\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43morder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mC\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccept_sparse\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcsr\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\n\u001b[1;32m   1314\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1315\u001b[0m     raw_predictions \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_raw_predict(X)\n\u001b[1;32m   1316\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m raw_predictions\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n",
      "File \u001b[0;32m~/miniconda3/envs/csi4106/lib/python3.8/site-packages/sklearn/base.py:585\u001b[0m, in \u001b[0;36mBaseEstimator._validate_data\u001b[0;34m(self, X, y, reset, validate_separately, **check_params)\u001b[0m\n\u001b[1;32m    582\u001b[0m     out \u001b[38;5;241m=\u001b[39m X, y\n\u001b[1;32m    584\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m no_val_X \u001b[38;5;129;01mand\u001b[39;00m check_params\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mensure_2d\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[0;32m--> 585\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_check_n_features\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    587\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m out\n",
      "File \u001b[0;32m~/miniconda3/envs/csi4106/lib/python3.8/site-packages/sklearn/base.py:400\u001b[0m, in \u001b[0;36mBaseEstimator._check_n_features\u001b[0;34m(self, X, reset)\u001b[0m\n\u001b[1;32m    397\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[1;32m    399\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m n_features \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_features_in_:\n\u001b[0;32m--> 400\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    401\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mX has \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mn_features\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m features, but \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    402\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mis expecting \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_features_in_\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m features as input.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    403\u001b[0m     )\n",
      "\u001b[0;31mValueError\u001b[0m: X has 16 features, but GradientBoostingClassifier is expecting 12 features as input."
     ]
    }
   ],
   "source": [
    "startTime = datetime.now()\n",
    "gradientPredictions = gradientBoost.predict(test_data)\n",
    "gradientPredictionsReport = classification_report(test_labels.flatten(), gradientPredictions, zero_division=0)\n",
    "print('Gradient boost train time:', datetime.now()-startTime )\n",
    "\n",
    "startTime = datetime.now()\n",
    "randomForrestPredictions = randomForrest.predict(test_data)\n",
    "randomForrestPredictionsReport = classification_report(test_labels.flatten(), randomForrestPredictions, zero_division=0)\n",
    "print('Random Forrest train time:', datetime.now()-startTime )\n",
    "\n",
    "startTime = datetime.now()\n",
    "decisionTreePredictions = decisionTree.predict(test_data)\n",
    "decisionTreePredictionsReport = classification_report(test_labels.flatten(), decisionTreePredictions, zero_division=0)\n",
    "print('Decision Tree train time:', datetime.now()-startTime )\n",
    "\n",
    "print('Gradient Boost Report')\n",
    "print(gradientPredictionsReport)\n",
    "print('Random Forrest Report')\n",
    "print(randomForrestPredictionsReport)\n",
    "print('Decision Tree Report')\n",
    "print(decisionTreePredictionsReport)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Metric                   | Gradient | Random Forrest | Decision Tree |\n",
    "|--------------------------|----------|----------------|---------------|\n",
    "| accuracy                 |    81%   |      85%       |     81%       |\n",
    "| precision (weighted avg) |    79%   |      76%       |     73%       |\n",
    "| recall (weighted avg)    |    81%   |      81%       |     78%       |\n",
    "| construction time        |          |                |               |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. (15 marks) Submit a 200 to 300 words summary explaining the actionable knowledge nuggets your team discovered. That is, you should explain what insights you obtained about the data, when investigating the models produced by the three algorithms. "
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
